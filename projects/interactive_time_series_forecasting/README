# Introduction
This project contains scripts that process 15 years of Stock Market data (obtained from a Kaggle dataset) and performs Data Wrangling and Predictive Modeling. The provided scripts use Dask for performing parallel computations on stock datasets.

# Deployment
## Running on Local machine
For running on local machine, setup a virtual environment and then install the recommended packages

## Data Upload to Amazon S3
You may use the provided scripts within ./scripts/misc/ to upload the provided data files to your Amazon S3 bucket

## Dask Yarn on Amazon EMR cluster
For deploying a Dask Yarn cluster on Amazon EMR, follow the Deployment section from Dask documentation:
https://yarn.dask.org/en/latest/aws-emr.html 

Note: You would have to allow inbound traffic to port 22 within your Master Node's security group policy (not recommended for production deployment)

Once, the EMR cluster is ready, SSH to your master node and access the scripts and data from your AWS S3 Bucket (that you previously uploaded).

## Changes to the scripts for execution on EMR cluster
Instead of using `LocalCluster()` within your Dask Client, you would replace it with `YarnCluster()`
All the path referenced to local file system must be replaced with correcponding references to paths on your Amazon S3 Bucket. For example, replace, './data/stock_data' with 's3://<bucket-name>/stock_data'

# Files Organization
## Scripts
./scripts/
./scripts/wrangling         <== Data Wrangling scripts that generate daily/quarterly data files
./scripts/modeling          <== Data Modeling scripts to be run on local / EMR cluster
./scripts/misc              <== Miscellaneous scripts like Data upload/download to/from Amazon S3

## Data
./data
./data/stock_data           <== Stock data text files (from Kaggle dataset)
./data/ticker_sector_name   <== Ticker to Sector mapping files (from web scraping)
./data/macrotrends          <== Generated by wrangling scripts
./data/wrangled_data_files  <== Generated by wrangling scripts
